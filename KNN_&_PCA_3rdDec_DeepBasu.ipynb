{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n"
      ],
      "metadata": {
        "id": "3V-RQwbXrpa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Introduction**  \n",
        "K-Nearest Neighbors KNN is a simple and intuitive supervised machine learning algorithm used for both classification and regression tasks. It is a non-parametric and instance-based method, meaning it does not build a model during training. Instead, it stores the training data and makes predictions by comparing new input samples with existing data points. The main idea behind KNN is that samples that are close to each other in feature space tend to have similar outputs.\n",
        "\n",
        "---\n",
        "\n",
        "**What is KNN**  \n",
        "KNN identifies the K closest data points from the training dataset when a new input point is given. These neighbors are determined using distance metrics such as Euclidean distance or Manhattan distance. Once these nearest points are found, the prediction is made based on their values. Since KNN delays computation until prediction time and simply stores the dataset during training, it is called a lazy learner.\n",
        "\n",
        "---\n",
        "\n",
        "**How KNN Works**  \n",
        "1. Choose a value for K.  \n",
        "2. Compute the distance between the new data point and all training samples.  \n",
        "3. Identify the K nearest neighbors based on distance.  \n",
        "4. Make the prediction based on these neighbors.\n",
        "\n",
        "Although the steps remain the same, the way predictions are made differs between classification and regression.\n",
        "\n",
        "---\n",
        "\n",
        "**KNN for Classification**  \n",
        "In classification, KNN predicts the class label of a new data point by applying **majority voting** among the K nearest neighbors. The class that appears most frequently among these neighbors becomes the predicted class.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "If K = 5 and the neighbors belong to classes A, A, B, A, and B, the predicted class is **A**.\n",
        "\n",
        "---\n",
        "\n",
        "**KNN for Regression**  \n",
        "In regression tasks, KNN predicts a continuous numerical value by taking the **average** or **weighted average** of the K nearest neighbors' values. This makes KNN flexible for both discrete and continuous output variables.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "If K = 3 and the neighbor values are 10, 12, and 14, the predicted value is:  \n",
        "(10 + 12 + 14) / 3 = **12**\n",
        "\n",
        "---\n",
        "\n",
        "**Distance Metrics Used in KNN**  \n",
        "- Euclidean Distance  \n",
        "- Manhattan Distance  \n",
        "- Minkowski Distance  \n",
        "- Hamming Distance (for categorical data)\n",
        "\n",
        "The choice of distance metric influences how neighbors are determined and affects accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "**Choosing the Value of K**  \n",
        "- A **small K** makes the model sensitive to noise and leads to overfitting.  \n",
        "- A **large K** smooths decision boundaries too much and may cause underfitting.  \n",
        "\n",
        "Cross-validation is commonly used to select the optimal K.\n",
        "\n",
        "---\n",
        "\n",
        "**Advantages of KNN**  \n",
        "- Simple to understand and easy to implement  \n",
        "- No training time required  \n",
        "- Works for both classification and regression  \n",
        "- Effective for smaller datasets  \n",
        "\n",
        "---\n",
        "\n",
        "**Limitations of KNN**  \n",
        "- Slow for large datasets due to distance calculations  \n",
        "- Sensitive to irrelevant and unscaled features  \n",
        "- Poor performance in high-dimensional spaces  \n",
        "- Requires storing the entire dataset in memory  \n",
        "\n",
        "---\n",
        "\n",
        "**Conclusion**  \n",
        "K-Nearest Neighbors is an intuitive algorithm that predicts outcomes based on similarity between data points. It uses majority voting for classification and averaging for regression. Although simple, its performance depends on choosing an appropriate K value, selecting a proper distance metric, and ensuring good feature scaling. When applied correctly, KNN can deliver reliable and interpretable results for a wide range of applications."
      ],
      "metadata": {
        "id": "jrZsASwOrsoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?"
      ],
      "metadata": {
        "id": "2Msudsv-uKtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Introduction**  \n",
        "The Curse of Dimensionality refers to a set of problems that arise when data is represented in a high number of dimensions (features). As dimensionality increases, data becomes sparse, distance measures become less meaningful, and algorithms that rely on proximity or density begin to perform poorly. This phenomenon significantly impacts KNN because KNN makes predictions entirely based on distance calculations.\n",
        "\n",
        "---\n",
        "\n",
        "**Definition of Curse of Dimensionality**  \n",
        "The Curse of Dimensionality describes how the behavior of data, distance, and volume changes as the number of features increases. In high-dimensional spaces, points that appear far apart in low-dimensional spaces become even more distant. Additionally, the amount of data required to represent the space grows exponentially, making it difficult to learn meaningful patterns.\n",
        "\n",
        "---\n",
        "\n",
        "**Why the Curse Occurs**  \n",
        "As the number of features increases:  \n",
        "- The volume of the feature space expands rapidly.  \n",
        "- Data becomes more spread out and sparse.  \n",
        "- All points begin to appear nearly equidistant from each other.  \n",
        "- Traditional distance metrics lose their ability to distinguish between near and far points.\n",
        "\n",
        "These conditions undermine algorithms that rely on neighborhood relationships.\n",
        "\n",
        "---\n",
        "\n",
        "**Impact on KNN Performance**  \n",
        "KNN depends heavily on identifying the closest neighbors using distance measures. However, in high-dimensional spaces, distances between data points become similar, making it difficult for KNN to find truly nearest neighbors.\n",
        "\n",
        "Effects include:  \n",
        "1. **Reduced Accuracy**  \n",
        "   KNN struggles to accurately classify or predict because the distinction between close and distant points weakens.\n",
        "\n",
        "2. **Increased Noise Sensitivity**  \n",
        "   Irrelevant or unimportant features distort distance calculations and mislead the algorithm.\n",
        "\n",
        "3. **Higher Computational Cost**  \n",
        "   Calculating distances in high-dimensional spaces becomes more expensive and time-consuming.\n",
        "\n",
        "4. **Overfitting**  \n",
        "   With many dimensions and few data points, KNN may fit noise instead of meaningful patterns.\n",
        "\n",
        "---\n",
        "\n",
        "**Example of the Problem**  \n",
        "In a low-dimensional space, such as 2D, points can be clearly clustered. However, in a 100-dimensional space, even points from the same class appear far apart. As a result, KNN may incorrectly classify points because the \"nearest\" neighbors may not truly be similar.\n",
        "\n",
        "---\n",
        "\n",
        "**How to Reduce the Curse of Dimensionality**  \n",
        "- **Feature Selection**: Removing irrelevant features to reduce dimensionality.  \n",
        "- **Dimensionality Reduction Techniques**: Methods like PCA, LDA, or t-SNE can compress data into fewer meaningful dimensions.  \n",
        "- **Normalization**: Scaling features can help distances behave more consistently.  \n",
        "\n",
        "These techniques help improve KNN’s accuracy and efficiency.\n",
        "\n",
        "In short, the Curse of Dimensionality causes distance metrics to lose meaning in high-dimensional spaces, leading to poor KNN performance. Because KNN relies entirely on distance-based neighbor selection, sparsity and noise in high-dimensional data greatly reduce its effectiveness."
      ],
      "metadata": {
        "id": "BpItd4ZtuUPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?"
      ],
      "metadata": {
        "id": "msWCZALzvEuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Introduction**  \n",
        "Principal Component Analysis PCA is a widely used dimensionality reduction technique that transforms high-dimensional data into a smaller set of uncorrelated components. These new components capture the maximum variance in the data. PCA is essential in machine learning for simplifying datasets, improving model performance, and reducing noise while preserving important information.\n",
        "\n",
        "---\n",
        "\n",
        "**What is PCA**  \n",
        "PCA is a mathematical transformation technique that converts original correlated features into a new set of uncorrelated variables called principal components. Each principal component is a linear combination of the original features, ordered such that the first principal component captures the highest variance, the second captures the next highest variance, and so on.\n",
        "\n",
        "PCA works by:  \n",
        "1. Standardizing the data.  \n",
        "2. Computing the covariance matrix.  \n",
        "3. Finding eigenvalues and eigenvectors of the covariance matrix.  \n",
        "4. Forming principal components based on the eigenvectors corresponding to the largest eigenvalues.\n",
        "\n",
        "The result is a reduced-dimensional representation that retains most of the important information from the original dataset.\n",
        "\n",
        "---\n",
        "\n",
        "**Purpose of PCA**  \n",
        "The main goal of PCA is to reduce dimensionality while keeping as much variance as possible. It helps in:  \n",
        "- Removing redundant or highly correlated features  \n",
        "- Reducing computational costs  \n",
        "- Improving model performance  \n",
        "- Visualizing high-dimensional data in 2D or 3D  \n",
        "\n",
        "---\n",
        "\n",
        "**What PCA Does Not Do**  \n",
        "PCA does not select original features but instead **creates new features** that are combinations of the original ones. These new features may not have direct interpretability but carry essential information.\n",
        "\n",
        "---\n",
        "\n",
        "**Difference Between PCA and Feature Selection**  \n",
        "\n",
        "**1. Nature of Output**  \n",
        "- **PCA:** Produces new transformed features called principal components.  \n",
        "- **Feature Selection:** Selects a subset of the original features without altering them.\n",
        "\n",
        "**2. Interpretability**  \n",
        "- **PCA:** Components are not easily interpretable because they are linear combinations of multiple features.  \n",
        "- **Feature Selection:** Retains original features, so interpretability remains intact.\n",
        "\n",
        "**3. Purpose**  \n",
        "- **PCA:** Reduces dimensionality by transforming features and minimizing redundancy.  \n",
        "- **Feature Selection:** Reduces dimensionality by selecting the most relevant features and removing irrelevant ones.\n",
        "\n",
        "**4. Relationship with Original Data**  \n",
        "- **PCA:** Creates new axes that maximize variance.  \n",
        "- **Feature Selection:** Keeps original features and discards the rest.\n",
        "\n",
        "**5. Handling Correlated Features**  \n",
        "- **PCA:** Handles correlation by combining highly correlated features into a single component.  \n",
        "- **Feature Selection:** Removes or keeps features but does not combine them.\n",
        "\n",
        "---\n",
        "\n",
        "*Example:*\n",
        "\n",
        "If you have 10 features and many of them are correlated, PCA may reduce the dataset to 3 principal components that capture 90 percent of the variance.  \n",
        "Feature selection, however, might choose 3 out of the original 10 features based on importance or correlation thresholds."
      ],
      "metadata": {
        "id": "2dF_BxHsvKFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n"
      ],
      "metadata": {
        "id": "gQrESEYuvgb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Introduction**  \n",
        "Eigenvalues and eigenvectors are fundamental mathematical concepts used in Principal Component Analysis PCA. They help identify the directions of maximum variance in the data and determine how much variance each direction captures. Understanding them is essential for understanding how PCA reduces dimensionality.\n",
        "\n",
        "---\n",
        "\n",
        "**What Are Eigenvalues**  \n",
        "Eigenvalues are scalar values that represent the amount of variance captured by each principal component. In the context of PCA, each eigenvalue corresponds to a specific eigenvector and indicates how significant that component is.  \n",
        "A larger eigenvalue means the corresponding component captures more variability in the data.\n",
        "\n",
        "---\n",
        "\n",
        "**What Are Eigenvectors**  \n",
        "Eigenvectors are direction vectors that show where the data varies the most. They define the new axes principal components onto which PCA projects the original data.  \n",
        "Each eigenvector points in the direction of maximum variance for that component.\n",
        "\n",
        "---\n",
        "\n",
        "**Role of Eigenvalues and Eigenvectors in PCA**  \n",
        "1. **Eigenvectors determine the direction of principal components**  \n",
        "   PCA rotates the original feature space to align with directions of maximum variance. These directions are given by eigenvectors of the covariance matrix.\n",
        "\n",
        "2. **Eigenvalues determine the importance of each principal component**  \n",
        "   Components with larger eigenvalues capture more information about data variation. Components with very small eigenvalues contribute little and can be removed during dimensionality reduction.\n",
        "\n",
        "3. **Ranking of Components**  \n",
        "   PCA ranks principal components based on eigenvalues. The first component has the largest eigenvalue and captures the highest variance.\n",
        "\n",
        "4. **Dimensionality Reduction**  \n",
        "   PCA keeps only the components with the highest eigenvalues. This reduces dimensions while preserving most of the information contained in the original dataset.\n",
        "\n",
        "---\n",
        "\n",
        "*Example:*\n",
        "  \n",
        "If the eigenvalues of a dataset are 5.2, 1.8, 0.3, and 0.02, it means:  \n",
        "- The first component explains most of the variance.  \n",
        "- The fourth component explains almost no variance and can be safely removed.\n",
        "\n",
        "The corresponding eigenvectors give the directions of these components in the feature space.\n",
        "\n",
        "---\n",
        "\n",
        "**Why They Are Important**  \n",
        "- They help identify which directions in the data have the most significant patterns.  \n",
        "- They determine how many components should be retained for effective dimensionality reduction.  \n",
        "- They reduce redundancy by capturing maximum variance using fewer components.  \n",
        "- They make PCA computationally efficient by focusing on the essential structure of the data.\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusion**  \n",
        "Eigenvalues and eigenvectors are central to PCA. Eigenvectors define the directions of new axes principal components, while eigenvalues show how much variance each component captures. Together, they allow PCA to reduce dimensionality effectively while preserving the most important information in the data."
      ],
      "metadata": {
        "id": "LQGc1UeFvmdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: How do KNN and PCA complement each other when applied in a single pipeline?"
      ],
      "metadata": {
        "id": "UHN4bCUQv2P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Introduction**  \n",
        "K Nearest Neighbors KNN is a distance-based algorithm that performs well when data is low-dimensional and well-structured. Principal Component Analysis PCA is a dimensionality reduction technique that transforms high-dimensional data into a smaller set of components. When used together in a pipeline, PCA improves the performance, speed, and reliability of KNN.\n",
        "\n",
        "---\n",
        "\n",
        "**Why PCA Is Needed Before KNN**  \n",
        "KNN relies entirely on distance calculations to find the nearest neighbors. In high-dimensional data, distances become less meaningful due to the curse of dimensionality. PCA reduces the number of dimensions while retaining the most important variance, making distance-based comparisons more accurate and meaningful for KNN.\n",
        "\n",
        "---\n",
        "\n",
        "**How PCA Improves KNN Performance**  \n",
        "\n",
        "1. **Reduces Noise and Irrelevant Features**  \n",
        "   PCA compresses data by focusing on the components with the highest variance. This removes noise and unimportant features, allowing KNN to work with cleaner, more informative data.\n",
        "\n",
        "2. **Mitigates the Curse of Dimensionality**  \n",
        "   By reducing dimensions, PCA ensures that neighbors in KNN are meaningful and not distorted by high-dimensional sparsity.\n",
        "\n",
        "3. **Improves Computational Efficiency**  \n",
        "   KNN becomes faster because fewer dimensions mean fewer distance calculations. This is especially valuable for large datasets.\n",
        "\n",
        "4. **Enhances Model Accuracy**  \n",
        "   With PCA removing redundant and correlated features, KNN can make more accurate predictions. Cleaner input features lead to better neighborhood relationships.\n",
        "\n",
        "5. **Reduces Overfitting**  \n",
        "   KNN can overfit when many irrelevant features influence distance calculations. PCA reduces this risk by compressing the feature space to the most important directions.\n",
        "\n",
        "---\n",
        "\n",
        "**Pipeline Workflow for PCA and KNN**  \n",
        "1. Standardize the data  \n",
        "2. Apply PCA to reduce dimensionality  \n",
        "3. Use the transformed principal components as input to KNN  \n",
        "4. Perform classification or regression using the K nearest neighbors  \n",
        "\n",
        "This pipeline ensures that KNN works on optimized, denoised, and compact data.\n",
        "\n",
        "---\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Suppose a dataset has 200 features, many of which are correlated or irrelevant. Without PCA, KNN may produce inaccurate results because distance calculations become unreliable.  \n",
        "After applying PCA, the dataset might be reduced to 20 principal components that capture most of the variance. KNN then performs more accurately and efficiently on these components."
      ],
      "metadata": {
        "id": "_jPXAzvcv69Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n"
      ],
      "metadata": {
        "id": "8gZG5u7iwWxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# KNN WITHOUT SCALING\n",
        "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scale.fit(X_train, y_train)\n",
        "pred_no_scale = knn_no_scale.predict(X_test)\n",
        "\n",
        "accuracy_no_scaling = accuracy_score(y_test, pred_no_scale)\n",
        "\n",
        "# KNN WITH SCALING\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "accuracy_with_scaling = accuracy_score(y_test, pred_scaled)\n",
        "\n",
        "# Print comparison\n",
        "print(\"Accuracy without scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with scaling:\", accuracy_with_scaling)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49cwCPEmwf5e",
        "outputId": "03f0338d-bef5-4c46-ae7c-279e30d78c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7222222222222222\n",
            "Accuracy with scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component."
      ],
      "metadata": {
        "id": "5tsR1sJTxpvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Feature scaling is important before PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA (all components)\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"Explained Variance Ratio of Each Principal Component:\\n\")\n",
        "for idx, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{idx+1}: {ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFSuQ1mDxsFr",
        "outputId": "48275b0a-1de5-4501-b7e5-231e0f551e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of Each Principal Component:\n",
            "\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n"
      ],
      "metadata": {
        "id": "WrRMn1I6yFyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 1. KNN ON ORIGINAL SCALED DATA\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# KNN on original scaled data\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# 2. KNN ON PCA-TRANSFORMED DATA (TOP 2 COMPONENTS)\n",
        "\n",
        "\n",
        "# Apply PCA to keep top 2 principal components\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# KNN on PCA-transformed data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# 3. Print comparison\n",
        "print(\"Accuracy of KNN on original scaled data:\", accuracy_original)\n",
        "print(\"Accuracy of KNN on PCA-transformed data (top 2 components):\", accuracy_pca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjrJUzl5yMjb",
        "outputId": "97706f93-57ae-49f8-83cd-a3614dee2ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of KNN on original scaled data: 0.9722222222222222\n",
            "Accuracy of KNN on PCA-transformed data (top 2 components): 0.9166666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n"
      ],
      "metadata": {
        "id": "pf4bsZ7ayh7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Scale the dataset\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# KNN with Euclidean Distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "pred_euclidean = knn_euclidean.predict(X_test)\n",
        "acc_euclidean = accuracy_score(y_test, pred_euclidean)\n",
        "\n",
        "# KNN with Manhattan Distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "pred_manhattan = knn_manhattan.predict(X_test)\n",
        "acc_manhattan = accuracy_score(y_test, pred_manhattan)\n",
        "\n",
        "# Print results\n",
        "print(\"Euclidean Distance Accuracy:\", acc_euclidean)\n",
        "print(\"Manhattan Distance Accuracy:\", acc_manhattan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb8S2DzVyjzG",
        "outputId": "53b2b829-19f7-4860-b0d6-84c33ace2748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean Distance Accuracy: 0.9722222222222222\n",
            "Manhattan Distance Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data"
      ],
      "metadata": {
        "id": "vsz6-Lsmy0yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=200,\n",
        "    n_features=2000,\n",
        "    n_informative=50,\n",
        "    n_redundant=50,\n",
        "    n_classes=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "pca_full = PCA()\n",
        "pca_full.fit(X_train_scaled)\n",
        "\n",
        "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "n_components_95 = int(np.argmax(cumulative_variance >= 0.95) + 1)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "max_components = int((cv.n_splits - 1) * X_train.shape[0] / cv.n_splits)\n",
        "n_components_95 = min(n_components_95, max_components)\n",
        "\n",
        "pca = PCA(n_components=n_components_95)\n",
        "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', pca),\n",
        "    ('knn', knn)\n",
        "])\n",
        "\n",
        "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='accuracy')\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(\"Number of components (>=95% or capped by CV limit):\", n_components_95)\n",
        "print(\"First 10 cumulative explained variance values:\", cumulative_variance[:10])\n",
        "print(\"Cross-validation accuracies:\", cv_scores)\n",
        "print(\"Mean CV accuracy:\", cv_scores.mean())\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo_-S_wuzGpn",
        "outputId": "9e94d228-6b43-4675-a593-7e826bb3c205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of components (>=95% or capped by CV limit): 128\n",
            "First 10 cumulative explained variance values: [0.01042577 0.02064903 0.03082016 0.0408319  0.05075927 0.06065697\n",
            " 0.07031695 0.07990645 0.08943378 0.09890067]\n",
            "Cross-validation accuracies: [0.375   0.46875 0.40625 0.5     0.5    ]\n",
            "Mean CV accuracy: 0.45\n",
            "Test accuracy: 0.35\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.21      0.27        14\n",
            "           1       0.30      0.23      0.26        13\n",
            "           2       0.36      0.62      0.46        13\n",
            "\n",
            "    accuracy                           0.35        40\n",
            "   macro avg       0.35      0.35      0.33        40\n",
            "weighted avg       0.35      0.35      0.33        40\n",
            "\n",
            "Confusion matrix:\n",
            " [[3 5 6]\n",
            " [2 3 8]\n",
            " [3 2 8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here,\n",
        "\n",
        "**Used PCA to reduce dimensionality**  \n",
        "The code applies PCA to the gene expression dataset after scaling the features. PCA transforms thousands of original gene features into a much smaller set of principal components that capture the most important variance. This reduces dimensionality and helps the model avoid overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "**Decided how many components to keep**  \n",
        "The number of PCA components is chosen using the cumulative explained variance. The code selects the smallest number of components that together explain at least 95% of the variance. This ensures that most meaningful information is kept while removing noise and redundancy from the data.\n",
        "\n",
        "---\n",
        "\n",
        "**Used KNN for classification post-dimensionality reduction**  \n",
        "After PCA reduces the dataset, the transformed features are passed into a KNN classifier within a pipeline. KNN operates on the lower-dimensional PCA space, making distance-based classification more accurate and less sensitive to irrelevant features. This directly implements KNN after dimensionality reduction.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The model is evaluated using multiple techniques:  \n",
        "- Stratified k-fold cross-validation for robust accuracy estimation  \n",
        "- Test accuracy on unseen data  \n",
        "- A classification report showing precision, recall, and F1-score  \n",
        "- A confusion matrix to analyze class-wise performance  \n",
        "\n",
        "These evaluation methods ensure the model is tested thoroughly and that results are not due to chance.\n",
        "\n",
        "---\n",
        "\n",
        "## Justifying this Pipeline for Real-World Biomedical Data\n",
        "\n",
        "Gene expression datasets in cancer research typically contain thousands of gene features but only a small number of patient samples. This makes traditional machine learning models overfit easily. PCA helps address this by compressing the high-dimensional gene expression data into a smaller set of meaningful components that capture underlying biological variation while filtering out noise. Using KNN after PCA provides a simple and interpretable classifier that works effectively in reduced-dimensional space, where distances between patients become more meaningful. Evaluation through cross-validation and testing ensures the model is reliable and generalizes well to new patient data. This combination of PCA and KNN forms a robust, scientifically sound pipeline that is suitable for biomedical applications where accuracy, stability, and interpretability are essential."
      ],
      "metadata": {
        "id": "INWJ4ETh4yrQ"
      }
    }
  ]
}