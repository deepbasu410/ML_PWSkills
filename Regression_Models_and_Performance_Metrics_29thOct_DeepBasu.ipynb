{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 : What is Simple Linear Regression (SLR)? Explain its purpose."
      ],
      "metadata": {
        "id": "y35dvZyZEYgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Simple Linear Regression (SLR)** is one of the most basic yet powerful statistical methods used to model and analyze the relationship between **two continuous variables** — one **independent variable (predictor)** and one **dependent variable (response)**. It assumes that the relationship between these variables can be expressed using a **straight-line equation**, given by:\n",
        "\n",
        "**Y = b₀ + b₁X + e**\n",
        "\n",
        "Where:  \n",
        "- **Y** = Dependent variable (the outcome or value to be predicted)  \n",
        "- **X** = Independent variable (the predictor or input variable)  \n",
        "- **b₀** = Intercept (the value of Y when X = 0)  \n",
        "- **b₁** = Slope (the amount of change in Y for a one-unit change in X)  \n",
        "- **e** = Error term (difference between observed and predicted values)\n",
        "\n",
        "The main **purpose of Simple Linear Regression** is to:  \n",
        "1. **Understand** the relationship between two variables.  \n",
        "2. **Quantify** how strongly one variable affects another.  \n",
        "3. **Predict** the value of the dependent variable based on the independent variable.  \n",
        "\n",
        "SLR works by fitting the **best possible straight line** through the data points using the **least squares method**, which minimizes the sum of squared differences between the observed and predicted values. This ensures that the fitted line represents the overall trend in the data as accurately as possible.\n",
        "\n",
        "**Interpretation of coefficients:**  \n",
        "- The **intercept (b₀)** indicates the predicted value of Y when X equals zero.  \n",
        "- The **slope (b₁)** shows how much Y changes for each one-unit increase in X. A positive slope means a direct relationship, while a negative slope indicates an inverse relationship.\n",
        "\n",
        "**Applications:**  \n",
        "- In **business**, it helps predict future sales based on advertising expenditure.  \n",
        "- In **economics**, it can forecast demand based on changes in price.  \n",
        "- In **education**, it can estimate student performance based on study hours.  \n",
        "- In **science and engineering**, it helps identify relationships between variables such as temperature and pressure.\n",
        "\n",
        "Overall, **Simple Linear Regression** is a foundational technique in both **statistics and machine learning**. It provides a clear, interpretable model for identifying trends, understanding cause-and-effect relationships, and making informed predictions. Though simple, it forms the basis for more complex regression models such as multiple and polynomial regression."
      ],
      "metadata": {
        "id": "EJyvgFs3EbZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What are the key assumptions of Simple Linear Regression?"
      ],
      "metadata": {
        "id": "g0JK30ZLFh-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Simple Linear Regression (SLR)**, like all statistical models, is based on certain key **assumptions** that must be satisfied for the model’s results to be reliable and accurate. These assumptions ensure that the estimated relationship between the independent and dependent variables is valid and that the conclusions drawn from the regression are meaningful.\n",
        "\n",
        "The **key assumptions of Simple Linear Regression** are as follows:\n",
        "\n",
        "1. **Linearity:**  \n",
        "   The relationship between the **independent variable (X)** and the **dependent variable (Y)** is assumed to be *linear*. This means that changes in X produce proportional changes in Y. If the relationship is non-linear, the linear regression model will not fit the data accurately.\n",
        "\n",
        "2. **Independence of Errors:**  \n",
        "   The residuals (errors) should be *independent* of each other. This means that the prediction errors for one observation should not influence the errors for another. Violation of this assumption often occurs in time-series data, where values are correlated over time.\n",
        "\n",
        "3. **Homoscedasticity (Constant Variance of Errors):**  \n",
        "   The variance of the residuals should remain *constant across all levels of X*. In other words, the spread of errors should be roughly the same for small and large values of the independent variable. If the variance changes (heteroscedasticity), it can lead to unreliable significance tests and biased standard errors.\n",
        "\n",
        "4. **Normality of Errors:**  \n",
        "   The residuals should be *normally distributed*. This assumption is especially important for hypothesis testing and constructing confidence intervals. If residuals are not normally distributed, the estimated coefficients and statistical tests may not be accurate.\n",
        "\n",
        "5. **No Multicollinearity (for Multiple Regression):**  \n",
        "   Although this primarily applies to **Multiple Linear Regression**, it is important to ensure that the predictor variable in SLR is not highly correlated with another variable. In SLR, since there is only one independent variable, this assumption is inherently satisfied.\n",
        "\n",
        "6. **No Autocorrelation:**  \n",
        "   The residuals should not exhibit *systematic patterns over time*. This is particularly important in time-dependent data. Autocorrelation violates the assumption of independent errors and can mislead the interpretation of regression results.\n",
        "\n",
        "7. **Measurement Accuracy:**  \n",
        "   Both the independent and dependent variables should be measured accurately. Errors in measurement can distort the relationship between X and Y, leading to biased parameter estimates.\n",
        "\n",
        "**Importance of These Assumptions:**  \n",
        "When these assumptions are met, the Simple Linear Regression model provides **unbiased, consistent, and efficient estimates** of the regression coefficients. If these assumptions are violated, it can result in misleading predictions, unreliable confidence intervals, and incorrect statistical conclusions.\n",
        "\n",
        "In short, checking these assumptions through diagnostic plots (like residual vs fitted plots, Q-Q plots, etc.) is a crucial step in regression analysis. Only when these assumptions hold true can the results of a Simple Linear Regression model be trusted for accurate prediction and interpretation."
      ],
      "metadata": {
        "id": "kgYspfbGFomH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: Write the mathematical equation for a simple linear regression model and explain each term.\n"
      ],
      "metadata": {
        "id": "xTpsn9GfF7cK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "The **mathematical equation** for a **Simple Linear Regression (SLR)** model is expressed as:\n",
        "\n",
        "**Y = b₀ + b₁X + e**\n",
        "\n",
        "This equation represents the relationship between a **dependent variable (Y)** and an **independent variable (X)**, assuming the relationship between them is *linear*.\n",
        "\n",
        "**Explanation of Each Term:**\n",
        "\n",
        "1. **Y (Dependent Variable):**  \n",
        "   It is the *output variable* or the *response* we are trying to predict or explain.  \n",
        "   Example: Sales, Marks, Salary, etc.  \n",
        "\n",
        "2. **X (Independent Variable):**  \n",
        "   It is the *predictor variable* or *input* that influences the dependent variable.  \n",
        "   Example: Advertising expenditure, Hours studied, Experience, etc.  \n",
        "\n",
        "3. **b₀ (Intercept or Constant Term):**  \n",
        "   It represents the *expected value of Y when X = 0*.  \n",
        "   In other words, it is the point where the regression line crosses the Y-axis.  \n",
        "   It indicates the baseline value of Y when there is no influence from X.  \n",
        "\n",
        "4. **b₁ (Slope or Regression Coefficient):**  \n",
        "   It measures the *rate of change* in Y for every *one-unit change* in X.  \n",
        "   - If **b₁ > 0**, there is a *positive relationship* (Y increases as X increases).  \n",
        "   - If **b₁ < 0**, there is a *negative relationship* (Y decreases as X increases).  \n",
        "   The slope thus determines the *direction* and *strength* of the relationship between X and Y.  \n",
        "\n",
        "5. **e (Error Term or Residual):**  \n",
        "   It represents the *unexplained variation* in Y — the difference between the *observed value* and the *predicted value* of Y.  \n",
        "   It accounts for random factors or influences not captured by the independent variable.  \n",
        "\n",
        "**Graphical Representation:**  \n",
        "In a scatter plot, the regression equation represents the *best-fitting straight line* through the data points, where:  \n",
        "- The line minimizes the sum of squared differences between the observed and predicted values (least squares method).  \n",
        "- The slope determines the tilt of the line, while the intercept sets its position on the Y-axis.\n",
        "\n",
        "*Example:*  \n",
        "If we are predicting a student’s score based on hours studied, the regression equation might be:  \n",
        "**Score = 35 + 5 * (Hours Studied)**  \n",
        "Here,  \n",
        "- **b₀ = 35** → Base score when study hours = 0  \n",
        "- **b₁ = 5** → Each additional study hour increases the score by 5 marks  \n"
      ],
      "metadata": {
        "id": "dj5QexKQGJMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: Provide a real-world example where simple linear regression can be applied."
      ],
      "metadata": {
        "id": "ZUgm4lcmGX1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "A **real-world example** of applying **Simple Linear Regression (SLR)** can be seen in the field of **marketing and sales prediction** — specifically, in estimating how **advertising expenditure** affects **product sales**.\n",
        "\n",
        "*Example:*  **Predicting Sales Based on Advertising Spend**\n",
        "\n",
        "A company wants to understand how much its **sales revenue (Y)** depends on the **amount spent on advertising (X)**. The goal is to find a linear relationship between these two variables and use it to predict future sales.\n",
        "\n",
        "**Scenario:**\n",
        "Suppose a retail company collects the following data for several months:\n",
        "- X = Amount spent on advertising (in thousand rupees)\n",
        "- Y = Monthly sales revenue (in thousand rupees)\n",
        "\n",
        "After plotting the data, it appears that as advertising spending increases, sales also tend to increase linearly. The company then applies the **Simple Linear Regression model**:\n",
        "\n",
        "**Y = b₀ + b₁X + e**\n",
        "\n",
        "Where:  \n",
        "- **Y** = Sales revenue  \n",
        "- **X** = Advertising expenditure  \n",
        "- **b₀** = Intercept (sales when advertising = 0)  \n",
        "- **b₁** = Slope (change in sales for a one-unit increase in advertising)  \n",
        "- **e** = Error term (unexplained variation)\n",
        "\n",
        "**Interpretation of Results:**\n",
        "Let’s assume the model output is:\n",
        "**Sales = 50 + 8 * (Advertising Spend)**  \n",
        "\n",
        "This means:  \n",
        "- When the company spends **₹0** on advertising, expected sales are **₹50,000** (the intercept).  \n",
        "- For every additional **₹1,000 spent on advertising**, sales are expected to increase by **₹8,000** (the slope).  \n",
        "\n",
        "**Insights and Usefulness:**\n",
        "- The company can use this model to **forecast future sales** based on different advertising budgets.  \n",
        "- It helps in **budget planning** by estimating the return on investment (ROI) from advertising.  \n",
        "- Management can use it to **evaluate marketing effectiveness** and make data-driven decisions.\n",
        "\n",
        "**Other Real-World Applications of SLR include:**\n",
        "1. Predicting **student performance** based on study hours.  \n",
        "2. Estimating **crop yield** based on rainfall or fertilizer use.  \n",
        "3. Predicting **house prices** based on size or location.  \n",
        "4. Forecasting **electricity consumption** based on temperature.  \n",
        "5. Estimating **employee attrition risk** based on years of service.\n",
        "\n",
        "In short, Simple Linear Regression provides an effective and interpretable way to model real-world relationships between two quantitative variables, enabling organizations to make informed predictions, optimize strategies, and identify key influencing factors."
      ],
      "metadata": {
        "id": "a_IToXiVG33d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: What is the method of least squares in linear regression?"
      ],
      "metadata": {
        "id": "H-Y98BUYHWFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "The **method of least squares** is a fundamental mathematical technique used in **linear regression** to find the *best-fitting line* through a set of data points. It determines the values of the regression coefficients (**b₀** and **b₁**) such that the line minimizes the total error between the observed and predicted values of the dependent variable.\n",
        "\n",
        "**Concept:**\n",
        "In Simple Linear Regression, the relationship between the dependent variable (Y) and the independent variable (X) is given by the equation:\n",
        "\n",
        "**Y = b₀ + b₁X + e**\n",
        "\n",
        "Where:\n",
        "- **Y** = Observed value (actual data)\n",
        "- **b₀** = Intercept of the regression line\n",
        "- **b₁** = Slope of the regression line\n",
        "- **X** = Independent variable\n",
        "- **e** = Error term (difference between observed and predicted Y)\n",
        "\n",
        "The **method of least squares** minimizes the *sum of squared residuals*, where each residual represents the vertical distance between the actual and predicted value of Y. Mathematically, this can be written as:\n",
        "\n",
        "**Minimize: Σ (Yᵢ − Ŷᵢ)²**\n",
        "\n",
        "where:\n",
        "- **Yᵢ** = Actual observed value  \n",
        "- **Ŷᵢ = b₀ + b₁Xᵢ** = Predicted value from the regression line  \n",
        "- The goal is to find the values of **b₀** and **b₁** that make this sum as small as possible.\n",
        "\n",
        "**Formulas for Coefficients:**\n",
        "The regression coefficients obtained using the least squares method are:\n",
        "\n",
        "**b₁ = Σ[(Xᵢ − X̄)(Yᵢ − Ȳ)] / Σ[(Xᵢ − X̄)²]**  \n",
        "**b₀ = Ȳ − b₁X̄**\n",
        "\n",
        "Where:\n",
        "- **X̄** = Mean of X values  \n",
        "- **Ȳ** = Mean of Y values  \n",
        "\n",
        "**Interpretation:**\n",
        "- The **slope (b₁)** represents how much Y changes for each unit change in X.  \n",
        "- The **intercept (b₀)** represents the predicted value of Y when X = 0.  \n",
        "\n",
        "**Why It’s Called “Least Squares”:**\n",
        "The method is called *least squares* because it minimizes the *sum of the squares* of the differences between observed and predicted values, ensuring that large errors are penalized more heavily than small ones.\n",
        "\n",
        "**Advantages of the Least Squares Method:**\n",
        "1. Provides an objective way to determine the best-fitting line.  \n",
        "2. Ensures the line passes through the center of the data (mean values of X and Y).  \n",
        "3. Produces unbiased and efficient estimates under standard regression assumptions.  \n",
        "4. Simple to compute and interpret.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "If we plot study hours (X) against test scores (Y), the least squares method will find the line that best represents the overall trend of scores increasing with study time. The line minimizes the total squared difference between the actual and predicted scores.\n",
        "\n",
        "To summararize, the **method of least squares** is the cornerstone of regression analysis. It ensures that the chosen regression line gives the smallest possible prediction errors, providing the most accurate and reliable model for understanding and forecasting relationships between variables."
      ],
      "metadata": {
        "id": "65HEqbmqHbvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: What is Logistic Regression? How does it differ from Linear Regression?"
      ],
      "metadata": {
        "id": "D_BziE-WH2dW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Logistic Regression** is a type of **supervised learning algorithm** used to model the relationship between one or more **independent variables (predictors)** and a **categorical dependent variable (outcome)**. Unlike **Linear Regression**, which predicts continuous numerical values, **Logistic Regression** predicts the *probability* that an observation belongs to a particular class (such as 0 or 1).\n",
        "\n",
        "It is widely used for **classification problems**, such as predicting whether an email is spam or not, whether a customer will buy a product, or whether a patient has a disease.\n",
        "\n",
        "---\n",
        "\n",
        "**Mathematical Form of Logistic Regression**\n",
        "\n",
        "The logistic regression model uses the **logistic (sigmoid) function** to map predicted values between 0 and 1.  \n",
        "The model is expressed as:\n",
        "\n",
        "**p = 1 / (1 + e^-(b₀ + b₁X))**\n",
        "\n",
        "Where:  \n",
        "- **p** = Probability that the dependent variable belongs to class 1  \n",
        "- **b₀** = Intercept  \n",
        "- **b₁** = Coefficient of the independent variable  \n",
        "- **X** = Independent variable  \n",
        "- **e** = Base of the natural logarithm (~2.718)\n",
        "\n",
        "The output **p** represents the probability of success (Y = 1), and **1 - p** represents the probability of failure (Y = 0).  \n",
        "A common threshold (like 0.5) is used to classify outcomes:\n",
        "- If **p ≥ 0.5**, predict class **1**\n",
        "- If **p < 0.5**, predict class **0**\n",
        "\n",
        "---\n",
        "\n",
        "**Difference Between Logistic and Linear Regression**\n",
        "\n",
        "| **Aspect** | **Linear Regression** | **Logistic Regression** |\n",
        "|-------------|------------------------|---------------------------|\n",
        "| **Type of Output** | Predicts *continuous numerical values* | Predicts *categorical outcomes* (probabilities between 0 and 1) |\n",
        "| **Equation Form** | Y = b₀ + b₁X | log(p / (1 - p)) = b₀ + b₁X |\n",
        "| **Nature of Relationship** | Models a *linear relationship* between X and Y | Models a *non-linear (S-shaped)* relationship using the logistic function |\n",
        "| **Error Measurement** | Uses *Mean Squared Error (MSE)* | Uses *Log-Loss* or *Cross-Entropy* |\n",
        "| **Assumptions About Residuals** | Errors are normally distributed | Residuals are not normally distributed; model uses probabilities |\n",
        "| **Use Case** | Regression (prediction of continuous outcomes) | Classification (prediction of categorical outcomes) |\n",
        "\n",
        "---\n",
        "\n",
        "*Example:*\n",
        "Suppose a bank wants to predict whether a loan applicant will default *(Yes/No)* based on income level.  \n",
        "- **Linear Regression** would give an output like 1.2 or -0.3, which are not valid probabilities.  \n",
        "- **Logistic Regression**, on the other hand, would output a probability like **0.85**, meaning there is an 85% chance that the applicant will default.\n",
        "\n",
        "---\n",
        "\n",
        "**Summary-**\n",
        "- **Linear Regression** is used when the dependent variable is *continuous* (e.g., predicting house prices, temperature, sales).  \n",
        "- **Logistic Regression** is used when the dependent variable is *categorical* (e.g., yes/no, pass/fail, churn/not churn).  \n",
        "- Logistic Regression transforms the linear combination of inputs using the *sigmoid function* to keep predictions within the range of 0 to 1, making it ideal for classification tasks.\n",
        "\n",
        "Thus, while both models aim to find relationships between variables, **Logistic Regression** specializes in predicting **probabilities and class memberships**, not continuous numerical outcomes."
      ],
      "metadata": {
        "id": "KKnGrtwJIBu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Name and briefly describe three common evaluation metrics for regression models.\n"
      ],
      "metadata": {
        "id": "2eOwQzrXIjnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Evaluating a **regression model** is essential to measure how well it predicts continuous outcomes. Several metrics are used to assess the performance and accuracy of regression models. The three most common evaluation metrics are **Mean Absolute Error (MAE)**, **Mean Squared Error (MSE)**, and **R-squared (R²)**.\n",
        "\n",
        "---\n",
        "\n",
        "**1. Mean Absolute Error (MAE)**\n",
        "\n",
        "**Definition:**  \n",
        "MAE measures the *average magnitude of errors* between predicted and actual values, without considering their direction (positive or negative).  \n",
        "It gives a straightforward measure of how far predictions are from the true values on average.\n",
        "\n",
        "**Formula:**  \n",
        "**MAE = (1/n) × Σ |Yᵢ − Ŷᵢ|**\n",
        "\n",
        "Where:  \n",
        "- **Yᵢ** = Actual value  \n",
        "- **Ŷᵢ** = Predicted value  \n",
        "- **n** = Number of observations  \n",
        "\n",
        "**Interpretation:**  \n",
        "- A *lower MAE* value indicates better model performance.  \n",
        "- It is easy to interpret since it uses the same units as the dependent variable.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "If the MAE is 3.5, it means that on average, predictions differ from actual values by 3.5 units.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Mean Squared Error (MSE)**\n",
        "\n",
        "**Definition:**  \n",
        "MSE measures the *average of squared differences* between predicted and actual values. It gives more weight to larger errors because the differences are squared.\n",
        "\n",
        "**Formula:**  \n",
        "**MSE = (1/n) × Σ (Yᵢ − Ŷᵢ)²**\n",
        "\n",
        "**Interpretation:**  \n",
        "- A *lower MSE* indicates better predictive accuracy.  \n",
        "- Since errors are squared, MSE penalizes large deviations more heavily, making it sensitive to outliers.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "If the MSE is 12.5, this means that on average, the squared difference between actual and predicted values is 12.5.\n",
        "\n",
        "---\n",
        "**3. R-squared (R²) – Coefficient of Determination**\n",
        "\n",
        "**Definition:**  \n",
        "R² measures the *proportion of variance* in the dependent variable that is explained by the independent variable(s) in the model.\n",
        "\n",
        "**Formula:**  \n",
        "**R² = 1 − (Σ (Yᵢ − Ŷᵢ)² / Σ (Yᵢ − Ȳ)²)**\n",
        "\n",
        "Where:  \n",
        "- **Ȳ** = Mean of actual values  \n",
        "\n",
        "**Interpretation:**  \n",
        "- **R²** ranges from **0 to 1**.  \n",
        "- An **R² of 1** means the model perfectly fits the data, while **R² of 0** means it does not explain any variation.  \n",
        "- Higher R² indicates better model performance.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "If R² = 0.85, it means that 85% of the variation in the dependent variable is explained by the model.\n",
        "\n",
        "---\n",
        "\n",
        "**Summary-**\n",
        "| **Metric** | **Measures** | **Ideal Value** | **Sensitivity** |\n",
        "|-------------|--------------|-----------------|-----------------|\n",
        "| **MAE** | Average absolute error | Closer to 0 | Low (robust to outliers) |\n",
        "| **MSE** | Average squared error | Closer to 0 | High (penalizes large errors) |\n",
        "| **R²** | Proportion of variance explained | Closer to 1 | Moderate |\n"
      ],
      "metadata": {
        "id": "hoJFpYonJBMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: What is the purpose of the R-squared metric in regression analysis?"
      ],
      "metadata": {
        "id": "O6fFFz0gJsd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "The **R-squared (R²)** metric, also known as the **Coefficient of Determination**, is a key statistical measure used in **regression analysis** to evaluate how well the independent variable(s) explain the variation in the dependent variable. It provides an overall indication of the **goodness of fit** of the regression model.\n",
        "\n",
        "---\n",
        "\n",
        "**Definition and Formula:**\n",
        "\n",
        "**R² = 1 − (Σ (Yᵢ − Ŷᵢ)² / Σ (Yᵢ − Ȳ)²)**\n",
        "\n",
        "Where:  \n",
        "- **Yᵢ** = Actual observed values  \n",
        "- **Ŷᵢ** = Predicted values from the regression model  \n",
        "- **Ȳ** = Mean of the actual values  \n",
        "\n",
        "---\n",
        "**Purpose of R-squared:**\n",
        "\n",
        "1. **Measures Model Fit:**  \n",
        "   R² indicates how well the regression line represents the observed data. A higher R² value means the model fits the data better.\n",
        "\n",
        "2. **Explains Variability:**  \n",
        "   It quantifies the **proportion of variance** in the dependent variable (**Y**) that is explained by the independent variable(s) (**X**).  \n",
        "   - For example, R² = 0.80 means that **80% of the variation** in Y can be explained by X, and the remaining 20% is due to random errors or factors not included in the model.\n",
        "\n",
        "3. **Evaluates Predictive Power:**  \n",
        "   R² helps assess how well the model can predict unseen data. A higher R² typically suggests stronger predictive capability, although it must be interpreted carefully.\n",
        "\n",
        "4. **Model Comparison:**  \n",
        "   When comparing multiple regression models built on the same dataset, R² can be used to determine which model better explains the data.\n",
        "\n",
        "---\n",
        "**Interpretation of R-squared Values:**\n",
        "\n",
        "| **R² Value** | **Interpretation** |\n",
        "|---------------|--------------------|\n",
        "| 0 | The model explains none of the variance in the dependent variable. |\n",
        "| 0 < R² < 0.5 | The model explains a small to moderate portion of the variance. |\n",
        "| 0.5 ≤ R² < 0.8 | The model has a reasonably good fit. |\n",
        "| 0.8 ≤ R² ≤ 1 | The model fits the data very well. |\n",
        "| 1 | Perfect fit (rare in real-world data). |\n",
        "\n",
        "---\n",
        "\n",
        "**Limitations of R-squared:**\n",
        "1. **Does not indicate causation** — A high R² does not mean that X causes Y.  \n",
        "2. **Sensitive to number of variables** — Adding more predictors always increases R², even if they are not meaningful.  \n",
        "3. **Does not measure bias or overfitting** — A model can have a high R² but still perform poorly on unseen data.  \n",
        "4. **Adjusted R-squared** is often preferred for multiple regression since it penalizes unnecessary predictors.\n",
        "\n",
        "---\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Suppose a regression model predicts house prices based on area.  \n",
        "If **R² = 0.85**, it means **85% of the variability** in house prices is explained by the model using the “area” variable, while 15% is due to other unexplained factors like location, design, or market trends.\n",
        "\n",
        "---\n",
        "\n",
        "**Summary-**\n",
        "\n",
        "The **purpose of R-squared** is to measure how well the regression model captures the variability of the dependent variable. It serves as a concise indicator of model effectiveness — the higher the R², the better the model explains and fits the data. However, it should always be interpreted alongside other metrics such as **Adjusted R², MAE, or MSE** to ensure a complete understanding of model performance."
      ],
      "metadata": {
        "id": "jmPrgClzKF16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept."
      ],
      "metadata": {
        "id": "hDQPRbEJKjy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"Slope (b1):\", model.coef_[0])\n",
        "print(\"Intercept (b0):\", model.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUwn-OE5PpeK",
        "outputId": "48612a62-d4f6-4ab3-a2fd-973593e2f355"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (b1): 0.6\n",
            "Intercept (b0): 2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: How do you interpret the coefficients in a simple linear regression model?"
      ],
      "metadata": {
        "id": "6t2tb2xJQdzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "In a **Simple Linear Regression (SLR)** model, the relationship between the **dependent variable (Y)** and the **independent variable (X)** is expressed using the equation:\n",
        "\n",
        "**Y = b₀ + b₁X + e**\n",
        "\n",
        "Where:  \n",
        "- **b₀** = Intercept (constant term)  \n",
        "- **b₁** = Coefficient or slope of X  \n",
        "- **e** = Error term  \n",
        "\n",
        "The **coefficients (b₀ and b₁)** are the most important parameters in a regression model, as they define the nature and strength of the relationship between X and Y.\n",
        "\n",
        "---\n",
        "\n",
        "**1. Intercept (b₀)**\n",
        "- The **intercept** represents the predicted value of the dependent variable (**Y**) when the independent variable (**X**) equals zero.  \n",
        "- It indicates the starting point of the regression line on the Y-axis.  \n",
        "- Mathematically, when X = 0, **Y = b₀**.  \n",
        "- The intercept helps establish the baseline level of Y before any influence from X.  \n",
        "\n",
        "*Example:*\n",
        "\n",
        "If the regression equation is **Y = 50 + 5X**, then **b₀ = 50** means that when X = 0, the expected value of Y is 50.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Slope (b₁)**\n",
        "- The **slope** (or regression coefficient) represents the *change in Y* for a *one-unit change in X*, assuming all other factors remain constant.  \n",
        "- It indicates both the **direction** and **magnitude** of the relationship between X and Y.  \n",
        "  - If **b₁ > 0**, there is a *positive relationship*: as X increases, Y increases.  \n",
        "  - If **b₁ < 0**, there is a *negative relationship*: as X increases, Y decreases.  \n",
        "- The absolute value of b₁ shows how sensitive Y is to changes in X.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "If **b₁ = 5**, it means that for every 1-unit increase in X, the predicted value of Y increases by 5 units.\n",
        "\n",
        "---\n",
        "**3. Overall Interpretation:**\n",
        "\n",
        "Together, **b₀ and b₁** define the regression line that best fits the data.  \n",
        "- **b₀** determines where the line crosses the Y-axis.  \n",
        "- **b₁** determines the slope or tilt of that line.  \n",
        "\n",
        "The model uses these coefficients to make predictions for Y based on any given value of X:\n",
        "**Ŷ = b₀ + b₁X**\n",
        "\n",
        "---\n",
        "\n",
        "**4. Example Interpretation:**\n",
        "\n",
        "Consider the regression equation:\n",
        "\n",
        "**Salary = 30,000 + 2,000 × Experience**\n",
        "\n",
        "Here:  \n",
        "- **Intercept (b₀ = 30,000):** When experience = 0, the predicted starting salary is ₹30,000.  \n",
        "- **Slope (b₁ = 2,000):** For every additional year of experience, salary increases by ₹2,000.\n",
        "\n",
        "Thus, both coefficients together describe how salary (Y) changes in response to experience (X).\n",
        "\n",
        "---\n",
        "\n",
        "**Summary-**\n",
        "\n",
        "The **coefficients in a simple linear regression model** provide a clear and interpretable description of the relationship between the independent and dependent variables.  \n",
        "- The **intercept** shows the baseline value of Y.  \n",
        "- The **slope** shows the rate and direction of change in Y with respect to X.  \n",
        "Together, they allow the model to make predictions, explain trends, and quantify the impact of one variable on another in a straightforward and interpretable way."
      ],
      "metadata": {
        "id": "NS2Uyk_AQg6g"
      }
    }
  ]
}