{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 : What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "7b7cbC8nLraO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Information Gain (IG)** is a metric used in Decision Trees to measure how effectively a feature separates the dataset into meaningful classes. It tells us how much “information” about the target variable is gained by splitting the data based on a particular feature. A higher Information Gain indicates a better splitting attribute, meaning it produces purer subsets with lower uncertainty.\n",
        "\n",
        "**Understanding Entropy**\n",
        "\n",
        "Entropy is used to measure the impurity or disorder within a dataset. It is calculated as:\n",
        "\n",
        "\\[\n",
        "Entropy(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\n",
        "\\]\n",
        "\n",
        "where \\(p_i\\) is the proportion of samples in class \\(i\\).\n",
        "\n",
        "- Entropy is 0 when all samples belong to the same class.\n",
        "- Entropy is high when classes are mixed and uncertain.\n",
        "\n",
        "**Information Gain Formula**\n",
        "\n",
        "Information Gain calculates the reduction in entropy after splitting:\n",
        "\\[\n",
        "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)\n",
        "\\]\n",
        "where \\(S_v\\) is the subset formed by splitting dataset \\(S\\) on feature \\(A\\).\n",
        "\n",
        "**How Information Gain is Used in Decision Trees**\n",
        "\n",
        "Decision Trees select the best feature for splitting by comparing the Information Gain of all candidate features. The feature with the highest Information Gain is chosen because it creates the most homogeneous (pure) child nodes. This process continues recursively, building the tree from top to bottom.\n",
        "\n",
        "- In the ID3 algorithm, IG is the main selection metric.\n",
        "- In algorithms like C4.5, IG is used along with Gain Ratio to avoid bias toward features with many distinct values.\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Suppose a dataset with a certain degree of impurity is split using feature A. If the child nodes become purer (less mixed), the entropy decreases. The difference between the original entropy and the weighted entropies of the children becomes the Information Gain. A high IG means the feature helps classify data better.\n",
        "\n",
        "**Limitations of Information Gain**\n",
        "\n",
        "- It is biased toward features with many unique values, such as IDs.\n",
        "- It may lead to overfitting if the tree grows too deep.\n",
        "- Numerical attributes require choosing optimal split thresholds, which increases computation.\n",
        "- Does not inherently account for issues like class imbalance.\n",
        "\n",
        "In short, Information Gain is a core concept in building effective Decision Trees. By measuring how much uncertainty is removed after a split, it helps the algorithm choose the most informative features at each step."
      ],
      "metadata": {
        "id": "cmvyz2tPLtph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases."
      ],
      "metadata": {
        "id": "MXaypN4DMnvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "Gini Impurity and Entropy are two impurity measures used in Decision Tree algorithms to determine how well a feature can split data into pure subsets. Both measure how mixed the classes are in a node, but they differ in formulation, sensitivity, and computational complexity.\n",
        "\n",
        "**Definition of Gini Impurity**  \n",
        "\n",
        "Gini Impurity measures the probability that a randomly chosen sample from a node would be incorrectly classified if it were labeled according to the class distribution.  \n",
        "\\[\n",
        "Gini = 1 - \\sum_{i=1}^{c} p_i^2\n",
        "\\]\n",
        "A Gini value of 0 means the node is perfectly pure (all samples belong to one class).\n",
        "\n",
        "**Definition of Entropy**\n",
        "\n",
        "Entropy measures the amount of uncertainty or randomness in the node.  \n",
        "\\[\n",
        "Entropy = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\n",
        "\\]\n",
        "Entropy is 0 when the node is perfectly pure and increases as class proportions become more evenly mixed.\n",
        "\n",
        "**Conceptual Difference**  \n",
        "\n",
        "- **Gini Impurity** focuses on misclassification probability and tends to create splits that isolate the most frequent class quickly.  \n",
        "- **Entropy** focuses on information content and is based on information theory, capturing overall uncertainty in the node.\n",
        "\n",
        "**Computational Difference**  \n",
        "\n",
        "- **Gini Impurity** is computationally faster because it does not require logarithmic calculations.  \n",
        "- **Entropy** requires log operations, making it slightly slower, especially on large datasets.\n",
        "\n",
        "**Sensitivity to Class Distribution**  \n",
        "\n",
        "- **Gini** is more sensitive to changes in class probabilities and tends to prefer splits that separate dominant classes.  \n",
        "- **Entropy** gives more balanced treatment to class changes and is more sensitive when probabilities are close.\n",
        "\n",
        "**When to Use Gini vs Entropy**\n",
        "\n",
        "- **Gini Impurity** is commonly used with the CART algorithm because it is faster and usually gives similar results to entropy.  \n",
        "- **Entropy** is used in ID3 and C4.5 and may be preferred when understanding the dataset in terms of information theory is important.  \n",
        "- In most real-world scenarios, both measures lead to similar tree structures, and the choice does not drastically change final performance.\n",
        "\n",
        "**Practical Example**  \n",
        "\n",
        "If two classes are equally mixed (for example, 50% each), both Gini and Entropy will reach their maximum impurity values. As one class becomes more dominant, both measures decrease, but Gini decreases slightly faster, leading to slightly different split preferences.\n",
        "\n",
        "**Conclusion**  \n",
        "\n",
        "Gini Impurity and Entropy both measure node impurity in Decision Trees but differ in computation and sensitivity. Gini is simpler and faster, while Entropy provides an information-theoretic view of uncertainty. Although their behaviors differ slightly, in practice they often produce similar results, and the choice depends on the specific algorithm or analytical preference."
      ],
      "metadata": {
        "id": "NTNke4y6MtgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3:What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "ugOIFjh6Nk7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Introduction**  \n",
        "Pre-pruning is a technique used to stop the growth of a Decision Tree early, before it becomes overly complex. Instead of allowing the tree to fully grow and then trimming it later, pre-pruning applies rules during the construction process to prevent unnecessary splitting. This helps reduce overfitting and improves the model’s generalization ability.\n",
        "\n",
        "**Definition of Pre-Pruning**  \n",
        "Pre-pruning (also known as early stopping) refers to imposing constraints while building the tree so that splitting stops when additional branches do not significantly improve the model. The idea is to halt the tree expansion when further splitting yields little gain in purity or performance.\n",
        "\n",
        "**Why Pre-Pruning Is Needed**  \n",
        "Decision Trees naturally tend to overfit by creating deep, complex trees that fit noise in the training data. Pre-pruning prevents this by avoiding splits that do not meaningfully reduce impurity. As a result, the tree is simpler, faster, and more interpretable.\n",
        "\n",
        "**Common Pre-Pruning Strategies**  \n",
        "- **Minimum Samples Split:** Stop splitting if the number of samples in a node is below a threshold.  \n",
        "- **Minimum Samples Leaf:** Require a minimum number of samples in each leaf node.  \n",
        "- **Maximum Depth:** Limit how deep the tree can grow.  \n",
        "- **Minimum Information Gain:** Allow a split only if the Information Gain exceeds a predefined threshold.  \n",
        "- **Maximum Number of Nodes:** Restrict total nodes to control complexity.\n",
        "\n",
        "**How Pre-Pruning Works**  \n",
        "At each node, the algorithm evaluates:  \n",
        "1. The best possible split based on impurity measures (Gini, Entropy).  \n",
        "2. Whether the improvement from this split meets the stopping criteria.  \n",
        "If the improvement is small or criteria are not met, the node becomes a leaf even if further splits exist.\n",
        "\n",
        "**Advantages of Pre-Pruning**  \n",
        "- Helps prevent overfitting by stopping early.  \n",
        "- Reduces training time and computational cost.  \n",
        "- Produces smaller, more interpretable trees.  \n",
        "- Useful when dealing with noisy or limited data.\n",
        "\n",
        "**Limitations of Pre-Pruning**  \n",
        "- Risk of underfitting if pruning is too aggressive.  \n",
        "- Hard to choose optimal thresholds for stopping criteria.  \n",
        "- Some splits may seem unhelpful early on but important later; pre-pruning might block them.\n",
        "\n",
        "  \n",
        "In short, Pre-pruning is an essential technique for controlling Decision Tree complexity by restricting growth during the training process. It improves generalization and interpretability, but must be applied carefully to avoid underfitting. When balanced correctly, pre-pruning leads to efficient and effective Decision Tree models."
      ],
      "metadata": {
        "id": "bE-id8-ENs96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_."
      ],
      "metadata": {
        "id": "XKSAmkdqN7TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Decision Tree with Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, score in zip(data.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-hXpHZyOFBa",
        "outputId": "1df8673b-d2c8-46ec-e096-7f96c4b2d791"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: What is a Support Vector Machine (SVM)?"
      ],
      "metadata": {
        "id": "-tN6sUz6OmMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Introduction**  \n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It is particularly powerful for binary classification and is known for its ability to handle high-dimensional data and create robust decision boundaries.\n",
        "\n",
        "**Definition of SVM**  \n",
        "A Support Vector Machine works by finding the optimal decision boundary, called a hyperplane, that best separates data points belonging to different classes. The goal is to maximize the margin, which is the distance between the hyperplane and the closest data points from each class. These closest points are known as support vectors, and they play a critical role in defining the decision boundary.\n",
        "\n",
        "**How SVM Works**  \n",
        "- SVM searches for the hyperplane that maximizes the margin between classes.  \n",
        "- Data points that lie closest to the hyperplane determine its position and orientation.  \n",
        "- If the data is linearly separable, SVM finds a straight-line (or plane) boundary.  \n",
        "- If the data is not linearly separable, SVM uses kernel functions to map the data into a higher-dimensional space where separation is possible.\n",
        "\n",
        "**Kernel Trick in SVM**  \n",
        "One of the major strengths of SVM is its ability to use kernels. A kernel is a mathematical function that transforms input data into a higher-dimensional space without explicitly computing the transformation. Common kernels include:\n",
        "- Linear Kernel  \n",
        "- Polynomial Kernel  \n",
        "- Radial Basis Function (RBF) Kernel  \n",
        "- Sigmoid Kernel  \n",
        "\n",
        "The kernel trick enables SVM to solve complex, non-linear problems effectively.\n",
        "\n",
        "**Types of SVM**  \n",
        "- **Linear SVM:** Used when the data can be separated by a straight line or plane.  \n",
        "- **Non-linear SVM:** Uses kernel functions for more complex data patterns.  \n",
        "- **Support Vector Regression (SVR):** Applies the SVM concept to regression tasks.\n",
        "\n",
        "**Advantages of SVM**  \n",
        "- Effective in high-dimensional spaces.  \n",
        "- Works well even when the number of features is greater than the number of samples.  \n",
        "- Robust to overfitting, especially with proper regularization (C parameter).  \n",
        "- Versatile due to the kernel trick.\n",
        "\n",
        "**Limitations of SVM**  \n",
        "- Computationally expensive for large datasets.  \n",
        "- Hard to tune due to choices of kernel, parameters like C and gamma.  \n",
        "- Not ideal for datasets with significant noise or overlapping classes.  \n",
        "- Output probabilities are not provided by default.\n",
        "\n",
        "**Conclusion**  \n",
        "Support Vector Machines are powerful and flexible algorithms capable of creating strong classification and regression models. By maximizing the margin and using the kernel trick, SVMs can handle both simple and complex datasets effectively, making them a widely used tool in machine learning applications."
      ],
      "metadata": {
        "id": "w5c7cgS0OwSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: What is the Kernel Trick in SVM?"
      ],
      "metadata": {
        "id": "IpQSKQdtO7Bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Introduction**  \n",
        "The Kernel Trick is a fundamental technique used in Support Vector Machines (SVMs) that enables the model to handle non-linearly separable data. It allows SVMs to create complex decision boundaries without increasing computational cost drastically.\n",
        "\n",
        "**Definition of the Kernel Trick**  \n",
        "The Kernel Trick refers to using a kernel function to implicitly transform data into a higher-dimensional feature space, where the classes become linearly separable. Instead of computing this transformation explicitly, the kernel function directly calculates the similarity between data points in the transformed space.\n",
        "\n",
        "**Why the Kernel Trick Is Needed**  \n",
        "Many real-world datasets cannot be separated using a straight line or linear decision boundary. Mapping these points into a higher-dimensional space can make separation possible. However, explicitly performing this transformation is computationally expensive. The Kernel Trick solves this by computing inner products in the higher-dimensional space without ever performing the transformation itself.\n",
        "\n",
        "**How the Kernel Trick Works**  \n",
        "- SVM uses a kernel function \\( K(x_i, x_j) \\) to replace the dot product in the feature space.  \n",
        "- The kernel function measures similarity in the transformed, higher-dimensional space.  \n",
        "- The SVM optimization problem remains computationally efficient because the transformation is never computed directly.\n",
        "\n",
        "**Common Kernel Functions**  \n",
        "- **Linear Kernel:**  \n",
        "  \\[\n",
        "  K(x_i, x_j) = x_i \\cdot x_j\n",
        "  \\]\n",
        "- **Polynomial Kernel:**  \n",
        "  \\[\n",
        "  K(x_i, x_j) = (x_i \\cdot x_j + c)^d\n",
        "  \\]\n",
        "- **RBF (Gaussian) Kernel:**  \n",
        "  \\[\n",
        "  K(x_i, x_j) = e^{-\\gamma \\|x_i - x_j\\|^2}\n",
        "  \\]\n",
        "- **Sigmoid Kernel:**  \n",
        "  \\[\n",
        "  K(x_i, x_j) = \\tanh(\\alpha x_i \\cdot x_j + c)\n",
        "  \\]\n",
        "\n",
        "Each kernel creates a different type of transformation suitable for different patterns in data.\n",
        "\n",
        "**Advantages of the Kernel Trick**  \n",
        "- Enables SVM to solve non-linear classification problems.  \n",
        "- Avoids computational costs of explicit high-dimensional transformations.  \n",
        "- Works well with complex data structures.  \n",
        "- Provides flexibility in modeling different types of decision boundaries.\n",
        "\n",
        "**Limitations**  \n",
        "- Choosing the right kernel and tuning parameters like \\( \\gamma \\), \\( C \\), and degree is challenging.  \n",
        "- Computationally expensive for very large datasets.  \n",
        "- May not perform well if data contains too much noise.\n",
        "\n",
        "\n",
        "In short, The Kernel Trick allows SVMs to efficiently handle non-linear patterns by transforming data into a higher-dimensional space implicitly. This capability makes SVM one of the most powerful and flexible algorithms in machine learning, especially for complex classification tasks."
      ],
      "metadata": {
        "id": "JVeTKIRGPA0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset."
      ],
      "metadata": {
        "id": "Z_ff9EvyPNo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "linear_pred = svm_linear.predict(X_test)\n",
        "linear_acc = accuracy_score(y_test, linear_pred)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "rbf_pred = svm_rbf.predict(X_test)\n",
        "rbf_acc = accuracy_score(y_test, rbf_pred)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"Accuracy using Linear Kernel:\", linear_acc)\n",
        "print(\"Accuracy using RBF Kernel:\", rbf_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwR2Y0SCPUhR",
        "outputId": "036fdc0c-022e-4970-ea12-b50503bd849a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using Linear Kernel: 0.9814814814814815\n",
            "Accuracy using RBF Kernel: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?"
      ],
      "metadata": {
        "id": "OQWrmiTaPnSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Introduction**  \n",
        "The Naïve Bayes classifier is a probabilistic machine learning algorithm based on Bayes’ Theorem. It is commonly used for classification tasks such as spam detection, text classification, medical diagnosis, and sentiment analysis. It is fast, simple, and surprisingly effective on high-dimensional data.\n",
        "\n",
        "**Definition of Naïve Bayes Classifier**  \n",
        "A Naïve Bayes classifier predicts the probability of each class for a given input and assigns the class with the highest probability. It applies Bayes’ Theorem:\n",
        "\\[\n",
        "P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n",
        "\\]\n",
        "where:  \n",
        "- \\(P(C|X)\\) = Posterior probability of class \\(C\\) given features \\(X\\)  \n",
        "- \\(P(X|C)\\) = Likelihood of features given the class  \n",
        "- \\(P(C)\\) = Prior probability of the class  \n",
        "- \\(P(X)\\) = Evidence (overall probability of the features)\n",
        "\n",
        "**Why It Is Called \"Naïve\"**  \n",
        "The algorithm is called \"Naïve\" because it assumes that all features are **conditionally independent** given the class label.  \n",
        "This means it treats each feature as if it has no influence on the others, which is rarely true in real data.\n",
        "\n",
        "Despite this unrealistic assumption, the classifier still performs remarkably well in many real-world scenarios.\n",
        "\n",
        "\n",
        "**Real-World Examples of Naïve Bayes**  \n",
        "- **Spam Filter:** Labels emails as spam or not spam by analyzing word frequencies.  \n",
        "- **Sentiment Analysis:** Classifies text as positive or negative based on word occurrences.  \n",
        "- **Medical Diagnosis:** Predicts diseases based on symptoms (e.g., fever, cough, fatigue).  \n",
        "- **Document Classification:** Categorizes news articles into categories such as sports, politics, or technology.\n",
        "\n",
        "**Types of Naïve Bayes Models**\n",
        "- **Gaussian Naïve Bayes:** Assumes features follow a normal distribution (useful for continuous numeric data).  \n",
        "- **Multinomial Naïve Bayes:** Used for word counts in text classification.  \n",
        "- **Bernoulli Naïve Bayes:** Suitable for binary features (word present or not present).\n",
        "\n",
        "**Advantages**\n",
        "- Simple to implement and very fast.  \n",
        "- Works well even with limited training data.  \n",
        "- Performs effectively on high-dimensional and sparse datasets.  \n",
        "- Naturally supports multi-class classification.\n",
        "\n",
        "**Limitations**\n",
        "- Independence assumption is unrealistic in many domains.  \n",
        "- Not ideal when features are highly correlated.  \n",
        "- Probability estimates can be less reliable for decision-making.\n",
        "\n",
        "**Conclusion**  \n",
        "Naïve Bayes is a simple yet powerful classifier that uses Bayes’ Theorem to compute class probabilities. It is called \"Naïve\" because it assumes feature independence, an assumption that simplifies computation but rarely holds true. Still, it is highly effective for text-based and large-scale classification tasks."
      ],
      "metadata": {
        "id": "iDudpLCdPsuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n"
      ],
      "metadata": {
        "id": "dpLxeh9zQYbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Introduction**  \n",
        "Naïve Bayes classifiers are a family of probabilistic classification algorithms based on Bayes’ Theorem and the assumption that features are conditionally independent given the class label. Different variants of Naïve Bayes exist to handle different types of data. The three most commonly used are Gaussian, Multinomial, and Bernoulli Naïve Bayes.\n",
        "\n",
        "---\n",
        "\n",
        "**Gaussian Naïve Bayes**  \n",
        "Gaussian Naïve Bayes is used when the features are **continuous numeric values** and are assumed to follow a **normal (Gaussian) distribution** for each class.\n",
        "\n",
        "The likelihood of a feature value is computed using the Gaussian probability density function:\n",
        "\n",
        "\\[\n",
        "P(x_i|C) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
        "\\]\n",
        "\n",
        "- Suitable for continuous variables like height, weight, temperature, exam scores, sensor readings, etc.  \n",
        "- Commonly used in medical data, real-valued classification tasks, and continuous feature datasets.\n",
        "\n",
        "*Example:*  \n",
        "Predicting whether a person has a disease based on continuous features like blood pressure or age.\n",
        "\n",
        "---\n",
        "\n",
        "**Multinomial Naïve Bayes**  \n",
        "Multinomial Naïve Bayes is used for **count-based features**, especially in text classification. It models the frequency of each feature (e.g., word counts or term frequencies).\n",
        "\n",
        "It assumes that:\n",
        "\n",
        "- Features represent counts (e.g., number of times a word appears).  \n",
        "- The distribution of features follows a multinomial distribution.\n",
        "\n",
        "This variant is widely used in **Natural Language Processing (NLP)** tasks.\n",
        "\n",
        "*Example:*  \n",
        "Classifying documents into topics such as sports, politics, or entertainment using word count vectors.\n",
        "\n",
        "---\n",
        "\n",
        "**Bernoulli Naïve Bayes**  \n",
        "Bernoulli Naïve Bayes is used when features are **binary variables** (0 or 1). It models whether a feature is present or absent in a sample.\n",
        "\n",
        "Each feature is treated as a Boolean indicator:\n",
        "\n",
        "- 1 → feature present  \n",
        "- 0 → feature absent  \n",
        "\n",
        "Bernoulli Naïve Bayes is suitable for binary-term representations such as the presence or absence of a word in text.\n",
        "\n",
        "*Example:*  \n",
        "Spam classification where features indicate whether certain keywords appear in an email (e.g., “free”, “win”, “offer”).\n",
        "\n",
        "---\n",
        "\n",
        "**Key Differences Between the Three**\n",
        "\n",
        "**1. Type of Data**  \n",
        "- **Gaussian NB:** Continuous numeric data.  \n",
        "- **Multinomial NB:** Discrete counts or frequency-based text features.  \n",
        "- **Bernoulli NB:** Binary features indicating presence/absence.\n",
        "\n",
        "**2. Probability Assumption**  \n",
        "- **Gaussian:** Features follow a Gaussian (normal) distribution.  \n",
        "- **Multinomial:** Features follow a multinomial distribution with counts.  \n",
        "- **Bernoulli:** Features follow a Bernoulli distribution (0/1).\n",
        "\n",
        "**3. Typical Use Cases**  \n",
        "- **Gaussian:** Medical data, sensor data, continuous measurements.  \n",
        "- **Multinomial:** Document classification, sentiment analysis, bag-of-words models.  \n",
        "- **Bernoulli:** Spam filtering, binary text features, simple NLP tasks.\n",
        "\n",
        "**4. Input Representation**  \n",
        "- **Gaussian:** Floating-point numbers.  \n",
        "- **Multinomial:** Integer counts (word frequencies).  \n",
        "- **Bernoulli:** Binary indicators (word present or not).\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusion**  \n",
        "Gaussian, Multinomial, and Bernoulli Naïve Bayes are tailored to different data formats. Gaussian is ideal for continuous data, Multinomial excels with count-based text data, and Bernoulli is effective for binary features. Selecting the correct variant based on the feature type ensures accurate and efficient classification."
      ],
      "metadata": {
        "id": "IInW144-RLzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets."
      ],
      "metadata": {
        "id": "7eS_VoRNRjaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy of Gaussian Naïve Bayes on Breast Cancer Dataset:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfyLrgNeRyEK",
        "outputId": "cd1c0449-e7b8-4dc9-f1c6-9c547f558d92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes on Breast Cancer Dataset: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}